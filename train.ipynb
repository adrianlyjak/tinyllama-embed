{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODOs\n",
    "\n",
    "- [x] Figure out why resumption is not working\n",
    "- Verify that loss function is implemented right \n",
    "- record / plot loss better and easy background training\n",
    "- [X] Research more QLoRA\n",
    "  - [ ] make sure that QLoRA is actually happening..\n",
    "  - RTFP\n",
    "  - [X] Find good solid example code with transformers\n",
    "  - Determine QLoRA parameters to experiment with and how\n",
    "    - datatype\n",
    "    - what bits\n",
    "    - r\n",
    "    - dropout\n",
    "    - target_modules\n",
    "    - lora alpha\n",
    "- Determine training parameters to experiment with\n",
    "  - AdamW RTFP\n",
    "  - tune adamW starting learning rate?\n",
    "  - Batch size. Should it vary over time?\n",
    "  - \n",
    "- get complicated and weird\n",
    "  - explore mining hard negatives\n",
    "  - explore synthesizing hard negatives\n",
    "  - synthesize more varied data\n",
    "  - generated data off of a target domain (easy-on fine tuning)\n",
    "- benchmark\n",
    "  - add inferrence code path\n",
    "  - figure out how to run model against an mteb benchmark\n",
    "- update code to be able to run it distributed in the cloud?\n",
    "  - related to deepspeed and accelerate\n",
    "    - https://arxiv.org/abs/2104.07857\n",
    "    - https://huggingface.co/docs/accelerate/usage_guides/deepspeed\n",
    "- [x] get a bigger graphics card"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import train\n",
    "\n",
    "base_model, tokenizer = train.load_model()\n",
    "model = train.load_model_for_training(base_model)\n",
    "\n",
    "dataset = train.prepare_dataset(tokenizer, seed)\n",
    "# Access the new train and test datasets\n",
    "train_dataset = dataset[\"train\"]\n",
    "eval_dataset = dataset[\"test\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [1, 7953, 2594, 12741, 29991, 2], 'attention_mask': [1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train_dataset[0]\n",
    "tokenizer(\"foo bar baz!\", add_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import TrainingArguments, IntervalStrategy\n",
    "from train import TinyEmbedTrainer\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    per_device_train_batch_size=4,\n",
    "    num_train_epochs=3,\n",
    "    logging_dir=\"./logs\",\n",
    "    log_level=\"info\",\n",
    "    logging_strategy=IntervalStrategy.STEPS,\n",
    "    gradient_checkpointing=True,\n",
    "    logging_steps=5,\n",
    "    save_steps=1000,\n",
    "    seed=seed,\n",
    ")\n",
    "\n",
    "trainer = TinyEmbedTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "trainer.train(resume_from_checkpoint=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from experiments import experiment_quantization\n",
    "import os\n",
    "\n",
    "experiment_quantization.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/freezer/dev/tinyllama-embed/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoConfig, AutoModel, AutoTokenizer\n",
    "from transformers.trainer_utils import get_last_checkpoint\n",
    "from train_main import load_trainer\n",
    "\n",
    "\n",
    "# trainer = load_trainer(40)\n",
    "# trainer._load_from_checkpoint(get_last_checkpoint(trainer.args.output_dir))\n",
    "# model = trainer.model\n",
    "# tokenizer = trainer.tokenizer\n",
    "model = AutoModel.from_pretrained('./results_44.0/checkpoint-40000/') #'TinyLlama/TinyLlama-1.1B-intermediate-step-1195k-token-2.5T')\n",
    "tokenizer = AutoTokenizer.from_pretrained('./results_44.0/checkpoint-40000/') #'TinyLlama/TinyLlama-1.1B-intermediate-step-1195k-token-2.5T') \n",
    "tokenizer.pad_token = tokenizer.unk_token\n",
    "tokenizer.pad_token_id = tokenizer.unk_token_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Retrieve critical reviews, analysis, and interpretations of the song 'Bohemian Rhapsody' by Queen.\n",
      "- The song 'Bohemian Rhapsody' by Queen has been widely praised for its unique structure and powerful vocals. Critics have analyzed the song's operatic elements and its impact on the rock genre. Many interpretations of the lyrics have been discussed, with some suggesting it reflects the inner turmoil of the songwriter. Overall, the song has been celebrated as a masterpiece of music history.\n",
      "- The song 'Bohemian Rhapsody' by Queen was released in 1975 and became a commercial success. It features a mix of rock and opera, and its lyrics have been the subject of much speculation. The song's popularity has led to numerous covers and references in popular culture. It remains a beloved classic and a staple of Queen's discography.\n",
      "- If your smartphone screen is frozen, there are several troubleshooting steps you can take to resolve the issue. First, try restarting the device by holding down the power button for a few seconds. If that doesn't work, you can try force restarting the phone by holding down both the power and volume buttons. Additionally, clearing the cache or performing a factory reset may help resolve the issue. For more detailed troubleshooting steps, refer to the user manual for your specific smartphone model.\n",
      "- Smartphone troubleshooting can be a complex process, especially when dealing with software issues. It's important to understand the various components of your device and how they interact with each other. By familiarizing yourself with the hardware and software of your smartphone, you can better troubleshoot any issues that may arise. Additionally, staying up to date with the latest firmware updates and security patches can help prevent potential problems in the future.\n",
      "- How to troubleshoot a frozen smartphone screen?\n",
      "- If your smartphone screen is frozen, there are several troubleshooting steps you can take to resolve the issue. First, try restarting the device by holding down the power button for a few seconds. If that doesn't work, you can try force restarting the phone by holding down both the power and volume buttons. Additionally, clearing the cache or performing a factory reset may help resolve the issue. For more detailed troubleshooting steps, refer to the user manual for your specific smartphone model.\n",
      "- Smartphone troubleshooting can be a complex process, especially when dealing with software issues. It's important to understand the various components of your device and how they interact with each other. By familiarizing yourself with the hardware and software of your smartphone, you can better troubleshoot any issues that may arise. Additionally, staying up to date with the latest firmware updates and security patches can help prevent potential problems in the future.\n",
      "- The user manual for Samsung Smart TV model XYZ123 provides detailed instructions on how to access and utilize the advanced settings feature. It includes step-by-step guides and troubleshooting tips to help you make the most of your TV's capabilities.\n",
      "- A review video of Samsung Smart TV model XYZ123 showcases its sleek design and impressive picture quality. The reviewer also discusses the various connectivity options and smart features, making it seem like a useful resource for understanding the TV's capabilities, but it does not provide the specific instructions the user is looking for.\n",
      "- How do I use the advanced settings on my Samsung Smart TV model XYZ123?\n",
      "- The user manual for Samsung Smart TV model XYZ123 provides detailed instructions on how to access and utilize the advanced settings feature. It includes step-by-step guides and troubleshooting tips to help you make the most of your TV's capabilities.\n",
      "- A review video of Samsung Smart TV model XYZ123 showcases its sleek design and impressive picture quality. The reviewer also discusses the various connectivity options and smart features, making it seem like a useful resource for understanding the TV's capabilities, but it does not provide the specific instructions the user is looking for.\n",
      "- The latest model of the iPhone has received rave reviews from customers, with many praising its improved camera quality, sleek design, and long battery life. The average rating on various platforms is 4.5 out of 5, making it a highly recommended choice for smartphone users.\n",
      "- While the iPhone is a popular choice, some customers have expressed dissatisfaction with the latest model, citing issues with software bugs and connectivity problems. Despite these concerns, the iPhone continues to receive positive feedback overall, with many users enjoying its advanced features and user-friendly interface.\n",
      "- What are the customer reviews and ratings for the latest model of the iPhone?\n",
      "- The latest model of the iPhone has received rave reviews from customers, with many praising its improved camera quality, sleek design, and long battery life. The average rating on various platforms is 4.5 out of 5, making it a highly recommended choice for smartphone users.\n",
      "- While the iPhone is a popular choice, some customers have expressed dissatisfaction with the latest model, citing issues with software bugs and connectivity problems. Despite these concerns, the iPhone continues to receive positive feedback overall, with many users enjoying its advanced features and user-friendly interface.\n",
      "- Are you looking for creative DIY projects that help reduce waste? Check out these 10 amazing crafts using recycled materials. From turning old jars into stylish vases to transforming cardboard into unique wall art, there's something for everyone to try!\n",
      "- Discover the art of upcycling and how it can transform your home. Learn how to repurpose old furniture and give it a new lease of life with these easy DIY projects. From restoring vintage chairs to creating custom storage solutions, you'll find everything you need to revamp your space.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pos</th>\n",
       "      <th>hard_neg</th>\n",
       "      <th>soft_neg1</th>\n",
       "      <th>soft_neg2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.736525</td>\n",
       "      <td>-0.141566</td>\n",
       "      <td>0.113576</td>\n",
       "      <td>-0.275082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.889012</td>\n",
       "      <td>-0.187596</td>\n",
       "      <td>0.008523</td>\n",
       "      <td>-0.197920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.833286</td>\n",
       "      <td>-0.211161</td>\n",
       "      <td>-0.007408</td>\n",
       "      <td>-0.217319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.896245</td>\n",
       "      <td>-0.234015</td>\n",
       "      <td>-0.021853</td>\n",
       "      <td>-0.175640</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        pos  hard_neg  soft_neg1  soft_neg2\n",
       "0  0.736525 -0.141566   0.113576  -0.275082\n",
       "1  0.889012 -0.187596   0.008523  -0.197920\n",
       "2  0.833286 -0.211161  -0.007408  -0.217319\n",
       "3  0.896245 -0.234015  -0.021853  -0.175640"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import train\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "dataset = train.prepare_dataset(tokenizer, 42)\n",
    "\n",
    "items = dataset[\"test\"].select(range(5)).to_list()\n",
    "\n",
    "results = []\n",
    "\n",
    "for a, b in zip(items, items[1:]):\n",
    "    query = a[\"query\"]\n",
    "    pos = a[\"pos\"]\n",
    "    hard_neg = a[\"neg\"]\n",
    "    soft_neg1 = b[\"pos\"]\n",
    "    soft_neg2 = b[\"neg\"]\n",
    "    embeddings = train.get_embeddings_from_text(\n",
    "        model, tokenizer, [query, pos, hard_neg, soft_neg1, soft_neg2]\n",
    "    )\n",
    "    query_embed = embeddings[0]\n",
    "    answer_embed = embeddings[1:]\n",
    "    similarities = F.cosine_similarity(\n",
    "        query_embed.repeat(len(answer_embed), 1), answer_embed\n",
    "    )\n",
    "    print(\"\\n\".join([ f\"- {x}\" for x in [query, pos, hard_neg, soft_neg1, soft_neg2]]))\n",
    "    results.append(\n",
    "        {\n",
    "            \"pos\": similarities[0].item(),\n",
    "            \"hard_neg\": similarities[1].item(),\n",
    "            \"soft_neg1\": similarities[2].item(),\n",
    "            \"soft_neg2\": similarities[3].item(),\n",
    "        }\n",
    "    )\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>There was a car crash on I-94. Two semi-trucks collided</th>\n",
       "      <th>I like waffles</th>\n",
       "      <th>I like hot dogs</th>\n",
       "      <th>I like french fries</th>\n",
       "      <th>The corporate lunch was awful</th>\n",
       "      <th>What happened in the news yesterday?</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>There was a car crash on I-94. Two semi-trucks...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.078175</td>\n",
       "      <td>0.214723</td>\n",
       "      <td>0.248861</td>\n",
       "      <td>0.076797</td>\n",
       "      <td>0.393608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I like waffles</td>\n",
       "      <td>0.078175</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.217257</td>\n",
       "      <td>0.334502</td>\n",
       "      <td>0.051545</td>\n",
       "      <td>0.107020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I like hot dogs</td>\n",
       "      <td>0.214723</td>\n",
       "      <td>0.217257</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.452492</td>\n",
       "      <td>0.166939</td>\n",
       "      <td>0.225391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I like french fries</td>\n",
       "      <td>0.248861</td>\n",
       "      <td>0.334502</td>\n",
       "      <td>0.452492</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.192974</td>\n",
       "      <td>0.184606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The corporate lunch was awful</td>\n",
       "      <td>0.076797</td>\n",
       "      <td>0.051545</td>\n",
       "      <td>0.166939</td>\n",
       "      <td>0.192974</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.040765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>What happened in the news yesterday?</td>\n",
       "      <td>0.393608</td>\n",
       "      <td>0.107020</td>\n",
       "      <td>0.225391</td>\n",
       "      <td>0.184606</td>\n",
       "      <td>0.040765</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  There was a car crash on I-94. Two semi-trucks...   \n",
       "1                                     I like waffles   \n",
       "2                                    I like hot dogs   \n",
       "3                                I like french fries   \n",
       "4                      The corporate lunch was awful   \n",
       "5               What happened in the news yesterday?   \n",
       "\n",
       "   There was a car crash on I-94. Two semi-trucks collided  I like waffles  \\\n",
       "0                                           1.000000              0.078175   \n",
       "1                                           0.078175              1.000000   \n",
       "2                                           0.214723              0.217257   \n",
       "3                                           0.248861              0.334502   \n",
       "4                                           0.076797              0.051545   \n",
       "5                                           0.393608              0.107020   \n",
       "\n",
       "   I like hot dogs  I like french fries  The corporate lunch was awful  \\\n",
       "0         0.214723             0.248861                       0.076797   \n",
       "1         0.217257             0.334502                       0.051545   \n",
       "2         1.000000             0.452492                       0.166939   \n",
       "3         0.452492             1.000000                       0.192974   \n",
       "4         0.166939             0.192974                       1.000000   \n",
       "5         0.225391             0.184606                       0.040765   \n",
       "\n",
       "   What happened in the news yesterday?  \n",
       "0                              0.393608  \n",
       "1                              0.107020  \n",
       "2                              0.225391  \n",
       "3                              0.184606  \n",
       "4                              0.040765  \n",
       "5                              1.000000  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from train import get_embeddings_from_text \n",
    "\n",
    "items = [\n",
    "  \"There was a car crash on I-94. Two semi-trucks collided\",\n",
    "  \"I like waffles\",\n",
    "  \"I like hot dogs\",\n",
    "  \"I like french fries\",\n",
    "  \"The corporate lunch was awful\",\n",
    "  \"What happened in the news yesterday?\",\n",
    "]\n",
    "embeddings = get_embeddings_from_text(model, tokenizer, items)\n",
    "\n",
    "data = []\n",
    "for text_a, embedding_a in zip(items, embeddings):\n",
    "  row = { \"text\": text_a }\n",
    "  data.append(row)\n",
    "  for text_b, embedding_b in zip(items, embeddings):\n",
    "    row[text_b] = F.cosine_similarity(embedding_a, embedding_b, dim=0).item()\n",
    "df = pd.DataFrame(data=data)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /mnt/freezer/huggingface-cache/hub/models--TinyLlama--TinyLlama-1.1B-intermediate-step-1195k-token-2.5T/snapshots/706bc2851338c4a89bb212e96ff23f9cc1ebde1d/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 2048,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 5632,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 22,\n",
      "  \"num_key_value_heads\": 4,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.37.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at /mnt/freezer/huggingface-cache/hub/models--TinyLlama--TinyLlama-1.1B-intermediate-step-1195k-token-2.5T/snapshots/706bc2851338c4a89bb212e96ff23f9cc1ebde1d/model.safetensors\n",
      "Instantiating LlamaModel model under default dtype torch.float16.\n",
      "Detected 4-bit loading: activating 4-bit loading for this model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at TinyLlama/TinyLlama-1.1B-intermediate-step-1195k-token-2.5T were not used when initializing LlamaModel: ['lm_head.weight']\n",
      "- This IS expected if you are initializing LlamaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LlamaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of LlamaModel were initialized from the model checkpoint at TinyLlama/TinyLlama-1.1B-intermediate-step-1195k-token-2.5T.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaModel for predictions without further training.\n",
      "loading file tokenizer.model from cache at /mnt/freezer/huggingface-cache/hub/models--TinyLlama--TinyLlama-1.1B-intermediate-step-1195k-token-2.5T/snapshots/706bc2851338c4a89bb212e96ff23f9cc1ebde1d/tokenizer.model\n",
      "loading file tokenizer.json from cache at /mnt/freezer/huggingface-cache/hub/models--TinyLlama--TinyLlama-1.1B-intermediate-step-1195k-token-2.5T/snapshots/706bc2851338c4a89bb212e96ff23f9cc1ebde1d/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at /mnt/freezer/huggingface-cache/hub/models--TinyLlama--TinyLlama-1.1B-intermediate-step-1195k-token-2.5T/snapshots/706bc2851338c4a89bb212e96ff23f9cc1ebde1d/special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at /mnt/freezer/huggingface-cache/hub/models--TinyLlama--TinyLlama-1.1B-intermediate-step-1195k-token-2.5T/snapshots/706bc2851338c4a89bb212e96ff23f9cc1ebde1d/tokenizer_config.json\n",
      "PyTorch: setting up devices\n",
      "max_steps is given, it will override any value given in num_train_epochs\n",
      "Loading model from ./results_40.0/checkpoint-1800.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from train_main import load_trainer\n",
    "\n",
    "from transformers.trainer_utils import get_last_checkpoint\n",
    "from train_main import load_trainer\n",
    "\n",
    "\n",
    "trainer = load_trainer(40)\n",
    "trainer._load_from_checkpoint(get_last_checkpoint(trainer.args.output_dir))\n",
    "model = trainer.model\n",
    "tokenizer = trainer.tokenizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import train\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "dataset: Dataset = trainer.eval_dataset\n",
    "batch = dataset.select(range(2)).to_list()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query1 tensor([ 0.0062,  0.0072,  0.0361,  ..., -0.1122, -0.0163,  0.0068],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "query1 input_ids tensor([    1,  4649, 29878,  2418, 12187, 21804, 29892,  7418, 29892,   322,\n",
      "         6613,   800,   310,   278,  4823,   525, 29933,  1148,   331,   713,\n",
      "          390,  4252,  1486, 29915,   491, 10470, 29889,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0])\n",
      "{'pos': 0.6921306848526001, 'neg': -0.014859418074289955}\n",
      "query2 input_ids tensor([    1,  4649, 29878,  2418, 12187, 21804, 29892,  7418, 29892,   322,\n",
      "         6613,   800,   310,   278,  4823,   525, 29933,  1148,   331,   713,\n",
      "          390,  4252,  1486, 29915,   491, 10470, 29889,     2,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0])\n",
      "query2 tensor([ 0.0025, -0.0006, -0.0046,  ..., -0.2503,  0.0048,  0.0159],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "embeddings tensor([[ 2.5206e-03, -6.2072e-04, -4.5602e-03,  ..., -2.5031e-01,\n",
      "          4.8285e-03,  1.5888e-02],\n",
      "        [ 3.1069e-03, -3.3699e-03, -3.9507e-03,  ..., -2.4982e-01,\n",
      "          2.2000e-03,  1.4809e-02],\n",
      "        [ 2.3662e-04, -3.0974e-03, -2.8845e-03,  ..., -2.4968e-01,\n",
      "          3.4581e-03,  1.6224e-02],\n",
      "        [ 8.1123e-03,  1.3104e-03, -5.1881e-03,  ..., -2.2883e-01,\n",
      "          5.0998e-03,  1.2725e-02],\n",
      "        [ 4.2961e-03, -1.0642e-03, -4.0027e-03,  ..., -2.4163e-01,\n",
      "          4.2917e-03,  1.3657e-02]], grad_fn=<DivBackward0>)\n",
      "- Retrieve critical reviews, analysis, and interpretations of the song 'Bohemian Rhapsody' by Queen.\n",
      "- The song 'Bohemian Rhapsody' by Queen has been widely praised for its unique structure and powerful vocals. Critics have analyzed the song's operatic elements and its impact on the rock genre. Many interpretations of the lyrics have been discussed, with some suggesting it reflects the inner turmoil of the songwriter. Overall, the song has been celebrated as a masterpiece of music history.\n",
      "- The song 'Bohemian Rhapsody' by Queen was released in 1975 and became a commercial success. It features a mix of rock and opera, and its lyrics have been the subject of much speculation. The song's popularity has led to numerous covers and references in popular culture. It remains a beloved classic and a staple of Queen's discography.\n",
      "- If your smartphone screen is frozen, there are several troubleshooting steps you can take to resolve the issue. First, try restarting the device by holding down the power button for a few seconds. If that doesn't work, you can try force restarting the phone by holding down both the power and volume buttons. Additionally, clearing the cache or performing a factory reset may help resolve the issue. For more detailed troubleshooting steps, refer to the user manual for your specific smartphone model.\n",
      "- Smartphone troubleshooting can be a complex process, especially when dealing with software issues. It's important to understand the various components of your device and how they interact with each other. By familiarizing yourself with the hardware and software of your smartphone, you can better troubleshoot any issues that may arise. Additionally, staying up to date with the latest firmware updates and security patches can help prevent potential problems in the future.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pos</th>\n",
       "      <th>hard_neg</th>\n",
       "      <th>soft_neg1</th>\n",
       "      <th>soft_neg2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.997785</td>\n",
       "      <td>0.997139</td>\n",
       "      <td>0.986794</td>\n",
       "      <td>0.992242</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        pos  hard_neg  soft_neg1  soft_neg2\n",
       "0  0.997785  0.997139   0.986794   0.992242"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collated = trainer._map_collate_fn(batch)\n",
    "\n",
    "# the embeddings here seem normal size\n",
    "trainer.compute_loss(model, collated)\n",
    "\n",
    "results = []\n",
    "\n",
    "for a, b in zip(batch, batch[1:]):\n",
    "    query = a[\"query\"]\n",
    "    pos = a[\"pos\"]\n",
    "    hard_neg = a[\"neg\"]\n",
    "    soft_neg1 = b[\"pos\"]\n",
    "    soft_neg2 = b[\"neg\"]\n",
    "    embeddings = train.get_embeddings_from_text(\n",
    "        model, tokenizer, [query, pos, hard_neg, soft_neg1, soft_neg2]\n",
    "    )\n",
    "    # these embeddings are way to small. Close to 0 (negative and positive)\n",
    "    query_embed = embeddings[0]\n",
    "    answer_embed = embeddings[1:]\n",
    "    similarities = torch.nn.functional.cosine_similarity(\n",
    "        query_embed.repeat(len(answer_embed), 1), answer_embed\n",
    "    )\n",
    "    print(\"\\n\".join([ f\"- {x}\" for x in [query, pos, hard_neg, soft_neg1, soft_neg2]]))\n",
    "    results.append(\n",
    "        {\n",
    "            \"pos\": similarities[0].item(),\n",
    "            \"hard_neg\": similarities[1].item(),\n",
    "            \"soft_neg1\": similarities[2].item(),\n",
    "            \"soft_neg2\": similarities[3].item(),\n",
    "        }\n",
    "    )\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "\n",
    "df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_embeddings tensor([[ 0.0261, -0.0123,  0.0158,  ...,  0.0009, -0.0283,  0.0222],\n",
      "        [ 0.0133,  0.0144, -0.0191,  ...,  0.0392, -0.0007,  0.0102],\n",
      "        [-0.0087, -0.0292, -0.0002,  ...,  0.0412, -0.0095,  0.0213],\n",
      "        [ 0.0299, -0.0183, -0.0254,  ...,  0.0640, -0.0162,  0.0198]],\n",
      "       grad_fn=<DivBackward0>)\n",
      "{'pos': 0.6921306848526001, 'neg': -0.014859418074289955}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6921306848526001"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.compute_loss(trainer.model, batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
