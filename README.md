An attempt at training TinyLlama for text retrieval embeddings.

Mistral Fine Tuned for Embeddings. 
	- [Tweet](https://twitter.com/andersonbcdefg/status/1742613575217156547) linking to recreation of synthetic data
- [Paper](https://arxiv.org/pdf/2401.00368.pdf) training mistral on synthetic data
	- [dataset](https://huggingface.co/datasets/andersonbcdefg/synthetic_retrieval_tasks)
	- [more dataset](https://huggingface.co/datasets/andersonbcdefg/synthetic_tuples_gpt35_turbo)
	- For that model, original training recipe  taken from here [https://arxiv.org/pdf/2310.08319.pdf](https://arxiv.org/pdf/2310.08319.pdf), training code [https://github.com/texttron/tevatron/tree/main/examples/repllama](https://github.com/texttron/tevatron/tree/main/examples/repllama)
- [TinyLlama](https://arxiv.org/html/2401.02385v1)